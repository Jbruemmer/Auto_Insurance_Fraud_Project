{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f15cb4",
   "metadata": {},
   "source": [
    "# Fraudulent Auto Insurance Claim Detection Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffae780",
   "metadata": {},
   "source": [
    "<hr ___ />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4104d8d",
   "metadata": {},
   "source": [
    "## Overview \n",
    "Accoding to Verisk Analytics, auto insurance fraud is a $29 billion problem. This is a result of omitted or misrepresented underwriting information and criminally inflated claims, leading to inadequate insurance and lower rates. But, there is no such thing as a free lunch. As you can imagine, this means that Insurance Companies are getting scammed out of money, and their customer's wallets are collectively taking the hit.  The goal of our model is to predict what auto insurance claims are likely to be overinflated. \n",
    "\n",
    "The Fraudulent Auto Insurance Claim Detection Model developed in this project could be of great value to any insurance company seeking to probe for and detect fraudulent or inflated insurance claims.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34632818",
   "metadata": {},
   "source": [
    "## Business Understanding  \n",
    "\n",
    "According to the FBI, the average(and most likely hard working, rule following) American family spends an extra $400 to $700 on insurance premiums every year because of insurance fraud.\n",
    "\n",
    "A major insurance company (think All-State, StateFarm, Geico, etc.) approached John and I a few weeks ago to help out their fraudulent claim division. Putting thier customers' needs first, they beleive they can save their company and their customers a substantial dollar amount if they had a better way to detect inflated and fraudlent insurance claims. \n",
    "\n",
    "There must be something in the air in the \"Windy City, becuase Chicago proper is one of our clients most fraudenlt territories in the United States. Before implementing nationally, they want to test a beta model in Illinois to guage efficacy. Utilizing the city of Chicago's transportation data portal, we were able to access information on every single documented car crash. Speficially, we used three sizable dataframes holding information about:\n",
    "\n",
    "1)The crash itself \n",
    "\n",
    "2)The people involved \n",
    "\n",
    "3)The vehicles involved "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1877ee",
   "metadata": {},
   "source": [
    "## Data Understanding and Preparation\n",
    "All the data used was gathered from the city of Chicago's \"Chicago Data Portal\". In order to get the most relevant data, we isolated the data taken between January of 2017 and January of 2022. We used three dataframes: 1) \"Traffic Crashes - Crashes\"   2) \"Traffic Crashes - People\"   3) \"Traffic Crashes - Cars\"\n",
    "\n",
    "\n",
    "\n",
    "Raw Data:\n",
    "\n",
    "Traffic Crashes - Crashes: 617,346 rows × 49 features\n",
    "\n",
    "Traffic Crashes - People: 777,348 rows × 11 features\n",
    "\n",
    "Traffic Crashes - Cars:  1,266,486 rows × 72 features\n",
    "\n",
    "\n",
    "\n",
    "Refined and merged data, before OneHotEncoding:  616067 rows × 41 columns\n",
    "\n",
    "\n",
    "Our target variable comes from the \"Traffic Crashes - Crashes dataset\". It was originally called \"DAMAGE\", and contained information on the cost of damages to the car, which could be one of three categories: \"Under 500 dollars\"(12 percent), \"500-1500 dollars\"(28 percent), and \"Over 1500 dollars(60 percent)\". \n",
    "\n",
    "In order to make our target binary and more balanced we combined the first two categories, making our new target: \"Under 1500 dollars\"(40 percent), \"Over 1500 dollars\"(60 percent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6505de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b8dd8",
   "metadata": {},
   "source": [
    "### Import, explore, and clean \"Crash\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Crash DataFrame \n",
    "crash_df = pd.read_csv('data/Traffic_Crashes_-_Crashes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e69932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade45f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crash_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b410e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Irrelevant columns \n",
    "crash_df.drop(['RD_NO', 'LANE_CNT','TRAFFIC_CONTROL_DEVICE','DEVICE_CONDITION', 'SEC_CONTRIBUTORY_CAUSE', 'CRASH_DATE_EST_I','TRAFFICWAY_TYPE','ALIGNMENT','ROAD_DEFECT','REPORT_TYPE','DATE_POLICE_NOTIFIED','STREET_NO','STREET_DIRECTION','STREET_NAME','PHOTOS_TAKEN_I','STATEMENTS_TAKEN_I','DOORING_I','WORK_ZONE_I','BEAT_OF_OCCURRENCE','WORK_ZONE_TYPE','WORKERS_PRESENT_I','INJURIES_TOTAL','INJURIES_FATAL','INJURIES_REPORTED_NOT_EVIDENT','INJURIES_NON_INCAPACITATING','INJURIES_NO_INDICATION','INJURIES_UNKNOWN','LATITUDE','LONGITUDE','LOCATION'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efbeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#crash_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9eb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill/Drop relevant nulls \n",
    "crash_df[\"INTERSECTION_RELATED_I\"].fillna(\"Unknown\", inplace=True)\n",
    "crash_df[\"NOT_RIGHT_OF_WAY_I\"].fillna(\"Unknown\", inplace=True)\n",
    "crash_df[\"HIT_AND_RUN_I\"].fillna(\"Unknown\", inplace=True)\n",
    "crash_df[\"MOST_SEVERE_INJURY\"].fillna(\"Unknown\", inplace=True)\n",
    "crash_df.dropna(subset=[\"INJURIES_INCAPACITATING\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create plot to show distribution of damage categories \n",
    "sns.histplot(crash_df['DAMAGE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed06864",
   "metadata": {},
   "source": [
    "### Import, explore, and clean \"People\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import People DataFrame \n",
    "people_df = pd.read_csv('data/Traffic_Crashes_-_People.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f4304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f1802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#people_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop irrelevant columns\n",
    "people_df.drop(['RD_NO', 'CRASH_DATE', 'SEAT_NO','CITY','STATE','ZIPCODE','DRIVERS_LICENSE_STATE','DRIVERS_LICENSE_CLASS','EJECTION','INJURY_CLASSIFICATION','HOSPITAL','EMS_AGENCY','EMS_RUN_NO','PEDPEDAL_ACTION','PEDPEDAL_VISIBILITY','PEDPEDAL_LOCATION','BAC_RESULT','BAC_RESULT VALUE','CELL_PHONE_USE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove nulls from relevant rows \n",
    "people_df.dropna(subset=[\"VEHICLE_ID\"], inplace=True)\n",
    "people_df.dropna(subset=[\"SEX\"], inplace=True)\n",
    "people_df.dropna(subset=[\"SAFETY_EQUIPMENT\"], inplace=True)\n",
    "people_df.dropna(subset=[\"AIRBAG_DEPLOYED\"], inplace=True)\n",
    "people_df.dropna(subset=[\"DRIVER_ACTION\"], inplace=True)\n",
    "people_df.dropna(subset=[\"DRIVER_VISION\"], inplace=True)\n",
    "people_df.dropna(subset=[\"PHYSICAL_CONDITION\"], inplace=True)\n",
    "people_df.dropna(subset=[\"AGE\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebf766",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1a292",
   "metadata": {},
   "source": [
    "### Import, explore, and clean \"Car\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf2e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "car_df = pd.read_csv('data/Traffic_Crashes_-_Vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa73d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#car_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912d04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#car_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369eba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new Car DataFrame with only relevant columns \n",
    "clean_car_df = car_df[['CRASH_RECORD_ID','UNIT_TYPE','MAKE','MODEL','VEHICLE_YEAR','VEHICLE_DEFECT','VEHICLE_TYPE','VEHICLE_USE','MANEUVER', 'TOWED_I','EXCEED_SPEED_LIMIT_I']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52909a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_car_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_car_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44066677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nulls \n",
    "clean_car_df.dropna(subset=[\"UNIT_TYPE\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"MAKE\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"MODEL\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"VEHICLE_YEAR\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"VEHICLE_DEFECT\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"VEHICLE_USE\"], inplace=True)\n",
    "clean_car_df.dropna(subset=[\"MANEUVER\"], inplace=True)\n",
    "clean_car_df[\"TOWED_I\"].fillna(\"Unknown\", inplace=True)\n",
    "clean_car_df[\"EXCEED_SPEED_LIMIT_I\"].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9ba02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_car_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2077c",
   "metadata": {},
   "source": [
    "### Merge Crash, People, and Car Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a512be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge crash data and people data \n",
    "crash_people_df = pd.merge(crash_df,people_df, how='left',left_on = 'CRASH_RECORD_ID', right_on = \"CRASH_RECORD_ID\", indicator=True)\n",
    "\n",
    "#remove duplicates \n",
    "crash_people_df.drop_duplicates(subset = 'CRASH_RECORD_ID', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename '_merge' column to 'Check', necessary for second merge \n",
    "crash_people_df.rename(columns = {'_merge':'Check'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge crash and people, and car DataFrames together(CPC) \n",
    "cpc_df = pd.merge(crash_people_df, clean_car_df, how='left',left_on = 'CRASH_RECORD_ID', right_on = \"CRASH_RECORD_ID\", indicator=True)\n",
    "\n",
    "#drop duplicates \n",
    "cpc_df.drop_duplicates(subset = 'CRASH_RECORD_ID', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1500d",
   "metadata": {},
   "source": [
    "####  Explore and clean new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89efaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ea642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a992b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ccce0",
   "metadata": {},
   "source": [
    "We predicted that the make of the car could be would be, to some extent, correlated with the cost of the repairs.  You would image the repairs for fender-bender on a Rolls-Royce would be far more expensive than, say, a Toyota.\n",
    "\n",
    "That being said, we also knew that we would have to OneHotEncode(OHE) every single make(which would've been several hundred new features), so we decided to just OHE the most popular 150 makes. \n",
    "\n",
    "Further, the Car-Model could've been even more valuable, but without more time we didn't think we could create an efficient model adding that many more features. As you can imgaine, nearly every car model built under the sun was on that list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with only the 150 most occuring \"Makes\", and an 'Other' \n",
    "TOP_MAKES = cpc_df['MAKE'].value_counts()\n",
    "threshold = 150\n",
    "cpc_df['TOP_MAKES'] = np.where(cpc_df['MAKE'].isin(TOP_MAKES.index[TOP_MAKES >= threshold ]), cpc_df['MAKE'], 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6e0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create plot for damage density \n",
    "damage_density = sns.histplot(crash_df['DAMAGE'], stat = 'density', color = '#212d74')\n",
    "damage_density.set_xlabel(\"Repair Cost\", fontsize = 15)\n",
    "damage_density.set_ylabel(\"Percent of Crashes\", fontsize = 15)\n",
    "damage_density.set_title(\"Cost Of Repair For Car Crashes\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fae1e",
   "metadata": {},
   "source": [
    "Here we see an pretty imbalanced distribution within our target feature. In order to make these more even, we decided to combine the two lowest categories into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5836ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use map function to create a binary target column \n",
    "#This helps to create more balanced dataset \n",
    "map = {\"OVER $1,500\":1,\"$501 - $1,500\": 0, \"$500 OR LESS\": 0}\n",
    "\n",
    "cpc_df[\"Target\"] = cpc_df[\"DAMAGE\"].map(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00377f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for balanced dataset\n",
    "#check to see the number of \"events\" vs \"non-events\" or most frequent outcome \n",
    "cpc_df[\"Target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae0a68",
   "metadata": {},
   "source": [
    "Here, we see that an \"event\" (1)(\"over $1,500\") occurs about 60% of the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29977a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cpc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea350da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop irrelevant columns \n",
    "cpc_df.drop(['PERSON_ID','CRASH_RECORD_ID','DAMAGE','CRASH_DATE','PERSON_TYPE', 'VEHICLE_ID','SAFETY_EQUIPMENT','DRIVER_VISION','Check','_merge','MODEL','MAKE','VEHICLE_DEFECT','VEHICLE_USE','EXCEED_SPEED_LIMIT_I'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b46ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nulls \n",
    "cpc_df.dropna(subset=[\"SEX\"], inplace=True)\n",
    "cpc_df.dropna(subset=[\"VEHICLE_YEAR\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e03fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cost_df =  cpc_df[cpc_df['Target'] == 1]\n",
    "low_cost_df = cpc_df[cpc_df['Target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6235276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize primary contributing causes \n",
    "sns.histplot(high_cost_df['PRIM_CONTRIBUTORY_CAUSE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745af41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_cost_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_low = low_cost_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts(normalize = True)[1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_high = high_cost_df['PRIM_CONTRIBUTORY_CAUSE'].value_counts(normalize = True)[1:6]\n",
    "top_5_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3852577",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_high.plot(kind = 'barh', title = \"Top 5 Primary Cause for High Cost Accidents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d135ee0",
   "metadata": {},
   "source": [
    "Looking at the top 5 primary causes for high cost and low cost accidnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top_5_high.plot(kind = 'barh', title = \"Top 5 Primary Cause for High Cost Accidents\", color = '#212d74')\n",
    "ax.set_xlabel(\"Percent of High Cost Accidents\")\n",
    "patches, labels = ax.get_legend_handles_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144aeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = top_5_low.plot(kind = 'barh', title = \"Top 5 Primary Cause for Low Cost Accidents\", color = '#212d74')\n",
    "ax.set_xlabel(\"Percent of Low Cost Accidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1fcf4",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f8fd",
   "metadata": {},
   "source": [
    "#### Test Train Split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a numeric feature dataframe \n",
    "#perform test train split \n",
    "\n",
    "numeric_df = cpc_df[['POSTED_SPEED_LIMIT','NUM_UNITS','INJURIES_INCAPACITATING',\n",
    "                     'CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH','AGE',\n",
    "                     'VEHICLE_YEAR', 'Target']]\n",
    "X = numeric_df.drop(\"Target\", axis=1)\n",
    "y = numeric_df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bd9b5",
   "metadata": {},
   "source": [
    "## 1st Model - \"Dummy Model\" (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931dc19",
   "metadata": {},
   "source": [
    "This model will predict the most frequent class for every observation. In other words, our model will \"guess\" the target that occurs most often. This will be a good baseline to compare future models against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate dummy model \n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model \n",
    "dummy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d16641",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model.predict(y_train)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa800ec",
   "metadata": {},
   "source": [
    "Here we see that guessing the most frequent event (1) every time, our model will be correct about 60% of the time(as this is the proportion of events(1) to nonevents(0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c760b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion matrix \n",
    "plot_confusion_matrix(dummy_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f469ed",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "#### Cross-validation will allow us to see how the model would do in generalizing to new data it's never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_val_score(dummy_model, X_train, y_train, cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d96e5",
   "metadata": {},
   "source": [
    "As we predicted, our model was correct approximately 60% of the time. \n",
    "\n",
    "\n",
    "To show the spread, we'll make a convenient class that can help us organize the model and the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf87191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} ± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_results = ModelWithCV(\n",
    "                        model=dummy_model,\n",
    "                        model_name='dummy',\n",
    "                        X=X_train, \n",
    "                        y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = dummy_model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "\n",
    "dummy_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Dummy Model\")\n",
    "\n",
    "plot_confusion_matrix(dummy_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(dummy_model, X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba42a52",
   "metadata": {},
   "source": [
    "## 2nd Model - Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f0686",
   "metadata": {},
   "source": [
    "Next we will create a logistic regression model and compare its performance.\n",
    "\n",
    "We're going to specifically avoid any regularization (the default) to see how the model does with little change. Set penalty paramter = 'none'  =  no regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting penalty = none means there is no regulaization, and thus we will not scale it \n",
    "simple_logreg_model = LogisticRegression(random_state=2021, penalty='none') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806263a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model and then predict \n",
    "simple_logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_logreg_model.predict(X_train)[200000:200050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bdade",
   "metadata": {},
   "source": [
    "Looking at 50 random samples, we see a mix of events and non-events this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d094be",
   "metadata": {},
   "source": [
    "###  2nd Model - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_logreg_results = ModelWithCV(\n",
    "                        model=simple_logreg_model,\n",
    "                        model_name='simple_logreg',\n",
    "                        X=X_train, \n",
    "                        y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving variable for convenience\n",
    "model_results = simple_logreg_results\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots()\n",
    "ax = model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "# Print CV results\n",
    "model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970de2ee",
   "metadata": {},
   "source": [
    "We see that with no regularization and default parameters, the model performs nearly the same as our basline model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(simple_logreg_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ca291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with Numeric Features Only\")\n",
    "\n",
    "plot_confusion_matrix(simple_logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1558937",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(simple_logreg_model, X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b7116",
   "metadata": {},
   "source": [
    "BUT, our ROC has improved. Our ROC curve now has an AUC of 0.56. This is better than our original model, but still not great. We hope by adding in more data preparation and feature engineering we can increase this more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b826f",
   "metadata": {},
   "source": [
    "## More Data Preparation \n",
    "\n",
    "This time we performed a train-test split that contains all of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f488787",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpc_df.drop(\"Target\", axis=1)\n",
    "y = cpc_df[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d8ebe",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_demo = MissingIndicator()\n",
    "\n",
    "indicator_demo.fit(X_train)\n",
    "\n",
    "indicator_demo.features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_demo.transform(X_train)[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# belowcreates a missing indicator column to help us see if something is\n",
    "#missing a value for a partiucal\n",
    "#column, --- NOT NECESSARY \n",
    "\n",
    "#what is essential !! is an imputer!! \n",
    "indicator = MissingIndicator(features=\"all\")\n",
    "indicator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb082dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_indicator_columns(X, indicator):\n",
    "    \"\"\"\n",
    "    Helper function for transforming features\n",
    "    \n",
    "    For every feature in X, create another feature indicating whether that feature\n",
    "    is missing. (This doubles the number of columns in X.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a 2D array of True and False values indicating whether a given feature\n",
    "    # is missing for that row\n",
    "    missing_array_bool = indicator.transform(X)\n",
    "    \n",
    "    # transform into 1 and 0 for modeling\n",
    "    missing_array_int = missing_array_bool.astype(int)\n",
    "    \n",
    "    # helpful for readability but not needed for modeling\n",
    "    missing_column_names = [col + \"_missing\" for col in X.columns]\n",
    "    \n",
    "    # convert to df so it we can concat with X\n",
    "    missing_df = pd.DataFrame(missing_array_int, columns=missing_column_names, index=X.index)\n",
    "    \n",
    "    return pd.concat([X, missing_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ff7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_missing_indicator_columns(X=X_train, indicator=indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d535b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate into numeric and categ. features \n",
    "numeric_feature_names = ['POSTED_SPEED_LIMIT','NUM_UNITS','INJURIES_INCAPACITATING',\n",
    "                           'CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH','AGE','VEHICLE_YEAR']\n",
    "categorical_feature_names = [c for c in cpc_df.columns if cpc_df[c].dtype == \"O\"]\n",
    "\n",
    "X_train_numeric = X_train[numeric_feature_names]\n",
    "X_train_categorical = X_train[categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing numeric columns using the mean for imputing, bc that is the default..would need to specify otherwise \n",
    "numeric_imputer = SimpleImputer()\n",
    "numeric_imputer.fit(X_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\") #here, we imputed using most freq for categorical vars.\n",
    "categorical_imputer.fit(X_train_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4526ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X, imputer):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and an imputer, use the imputer to fill in all\n",
    "    missing values in the DataFrame\n",
    "    \"\"\"\n",
    "    imputed_array = imputer.transform(X)\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=X.columns, index=X.index)\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b56e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = impute_missing_values(X_train_numeric, numeric_imputer)\n",
    "X_train_categorical = impute_missing_values(X_train_categorical, categorical_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = pd.concat([X_train_numeric, X_train_categorical], axis=1)\n",
    "X_train_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6613242",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(numeric_feature_names + categorical_feature_names, axis=1)\n",
    "X_train = pd.concat([X_train_imputed, X_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmed there were no null values before OneHotEncoding\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d449b78",
   "metadata": {},
   "source": [
    "### One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpc_df.drop(columns='Target')\n",
    "y = cpc_df[\"Target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03884a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_names = [c for c in cpc_df.columns if cpc_df[c].dtype == \"O\"]\n",
    "numerical_feature_names = ['POSTED_SPEED_LIMIT','NUM_UNITS','INJURIES_INCAPACITATING',\n",
    "                           'CRASH_HOUR','CRASH_DAY_OF_WEEK','CRASH_MONTH','AGE','VEHICLE_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_and_concat_feature_train(X_train, feature_name):\n",
    "    \"\"\"\n",
    "    Helper function for transforming training data.  It takes in the full X dataframe and\n",
    "    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n",
    "    with that feature transformed into multiple columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # make a one-hot encoder and fit it to the training data\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    single_feature_df = X_train[[feature_name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    \n",
    "    # call helper function that actually encodes the feature and concats it\n",
    "    X_train = encode_and_concat_feature(X_train, feature_name, ohe)\n",
    "    \n",
    "    return ohe, X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_and_concat_feature(X, feature_name, ohe):\n",
    "    \"\"\"\n",
    "    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n",
    "    in both training and testing steps.  Takes in the full X dataframe, feature name, \n",
    "    and encoder, and returns the dataframe with that feature transformed into multiple\n",
    "    columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # create new one-hot encoded df based on the feature\n",
    "    single_feature_df = X[[feature_name]]\n",
    "    feature_array = ohe.transform(single_feature_df).toarray()\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0], index=X.index)\n",
    "    \n",
    "    # drop the old feature from X and concat the new one-hot encoded df\n",
    "    X = X.drop(feature_name, axis=1)\n",
    "    X = pd.concat([X, ohe_df], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37358f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "\n",
    "for categorical_feature in categorical_feature_names:\n",
    "    ohe, X_train = encode_and_concat_feature_train(X_train, categorical_feature)\n",
    "    encoders[categorical_feature] = ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35203de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53adb1",
   "metadata": {},
   "source": [
    "### Decision Tree - For Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiate Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=13, random_state=42)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "CV_results = cross_val_score(dt,X_train,y_train,cv=5)\n",
    "CV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dda870",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dt,X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of feature importance \n",
    "list = {}\n",
    "for fi, feature in zip(dt.feature_importances_,X_train):\n",
    "    list.update({fi:feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order by most important \n",
    "import collections\n",
    "od = collections.OrderedDict(sorted(list.items(),reverse=True))\n",
    "od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize \n",
    "n_features = dt.n_features_\n",
    "plt.figure(figsize=(15, 70))\n",
    "plt.barh(range(n_features), dt.feature_importances_);\n",
    "plt.yticks(np.arange(n_features), X_train.columns.values, fontsize = 12) \n",
    "plt.xlabel('Feature importance', fontsize = 20)\n",
    "plt.ylabel('Features', fontsize = 20)\n",
    "plt.title('FSM Feature Importance', fontsize = 20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc046b",
   "metadata": {},
   "source": [
    "With more time, we would impute all of our \"unknown\" data and determine featuer importance again. Based on the results, we would remove the the unimportant features and focus on the most important ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d346e",
   "metadata": {},
   "source": [
    "## \"3rd Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4da422",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021, penalty='none')\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more iterations\n",
    "logreg_model_more_iterations = LogisticRegression(\n",
    "                                                random_state=2021, \n",
    "                                                penalty='none', \n",
    "                                                max_iter=100\n",
    ")\n",
    "logreg_model_more_iterations.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ea54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#higher tolerance (C-parameter is inverse of regularization strength)\n",
    "#higher tolerance means that our models will stop training earlier (when predictors and \n",
    "#true values are not as close as they could be).\n",
    "logreg_model_higher_tolerance = LogisticRegression(\n",
    "                                                random_state=2021, \n",
    "                                                penalty='none', \n",
    "                                                tol=25\n",
    ")\n",
    "logreg_model_higher_tolerance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692ddc5",
   "metadata": {},
   "source": [
    "## 3rd Model - Model Evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958c2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "axes[0].set_title(\"More Iterations\")\n",
    "axes[1].set_title(\"Higher Tolerance\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model_more_iterations, X_train, y_train,\n",
    "                      ax=axes[0], cmap=\"plasma\")\n",
    "plot_confusion_matrix(logreg_model_higher_tolerance, X_train, y_train,\n",
    "                      ax=axes[1], cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc48ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model_more_iterations_results = ModelWithCV(\n",
    "                                        logreg_model_more_iterations,\n",
    "                                        'more_iterations',\n",
    "                                        X_train,\n",
    "                                        y_train\n",
    ")\n",
    "    \n",
    "logreg_model_higher_tolerance_results = ModelWithCV(\n",
    "                                        logreg_model_higher_tolerance,\n",
    "                                        'higher_tolerance',\n",
    "                                        X_train,\n",
    "                                        y_train\n",
    ")\n",
    "\n",
    "model_results = [\n",
    "    logreg_model_more_iterations_results,\n",
    "    logreg_model_higher_tolerance_results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf4797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "for ax, result in zip(axes, model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "    result.print_cv_summary()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3eca7e",
   "metadata": {},
   "source": [
    "Here we see a slight improvement from our previous scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_roc_curve(logreg_model_more_iterations, X_train, y_train, \n",
    "               name='logreg_model_more_iterations', ax=ax)\n",
    "plot_roc_curve(logreg_model_higher_tolerance, X_train, y_train, \n",
    "               name='logreg_model_higher_tolerance', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca89525",
   "metadata": {},
   "source": [
    "Here, we see a major improvememnt! Could be result of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b75c0",
   "metadata": {},
   "source": [
    "# 4th Model - After Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8fb40",
   "metadata": {},
   "source": [
    "## More Data Preparation - Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b50e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(X, scaler):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a fitted scaler, use the scaler to scale all of the features\n",
    "    \"\"\"\n",
    "    scaled_array = scaler.transform(X)\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=X.columns, index=X.index)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_values(X_train, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f980c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1f330",
   "metadata": {},
   "source": [
    "Now that we have scaled data, lets see how well our logistic regression model fits without adjusting any hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021)\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with All Features, Scaled\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            'all_features',\n",
    "                            X_train,\n",
    "                            y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving variable for convenience\n",
    "model_results = all_features_results\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots()\n",
    "ax = model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "# Print CV results\n",
    "model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47cc91",
   "metadata": {},
   "source": [
    "We see that scaling improved our accuracy scores. We also see below that the AUC increased slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc27a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(logreg_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a9c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(list(zip(X_train.columns, logreg_model.coef_[0])),\n",
    "#        key=lambda x: abs(x[1]), reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so now lets increase the regularization - the correct the overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ef51e",
   "metadata": {},
   "source": [
    "## Hyperparameter Adjustment\n",
    "\n",
    "### Different Regularization Strengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5343b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = [all_features_results]\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for c in C_values:\n",
    "    logreg_model = LogisticRegression(random_state=2021, C=c)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    # Save Results\n",
    "    new_model_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            f'all_features_c{c:e}',\n",
    "                            X_train,\n",
    "                            y_train\n",
    "    )\n",
    "    model_results.append(new_model_results)\n",
    "    new_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db122a",
   "metadata": {},
   "source": [
    "Here, we don't see any any significant improvement in accuracy with C-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2591fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=3, nrows=2, sharey='all', figsize=(18, 12))\n",
    "\n",
    "for ax,result in zip(axes.ravel(),model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = [all_features_results]\n",
    "all_features_cross_val_score = all_features_results.cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78336442",
   "metadata": {},
   "source": [
    "### Different Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fdb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = [all_features_results]\n",
    "all_features_cross_val_score = all_features_results.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\")\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f85033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe606f",
   "metadata": {},
   "source": [
    "No major difference in the scores. Let's try adding some more regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\", C=0.01)\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_C:0.01',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7abdda",
   "metadata": {},
   "source": [
    "Slightly better, if any. Lets try another different type of penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\")\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a497fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save for later comparison\n",
    "# model_results.append(\n",
    "#     ModelWithCV(\n",
    "#         logreg_model, \n",
    "#         'solver:liblinear_penalty:l1',\n",
    "#         X_train,\n",
    "#         y_train\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Plot both all_features vs new model\n",
    "# f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "# model_results[0].plot_cv(ax=axes[0])\n",
    "# model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038df69",
   "metadata": {},
   "source": [
    "This took too long to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\", C=0.01)\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8899b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_penalty:l1_C:0.01',\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[0].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c746c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old:\", all_features_cross_val_score)\n",
    "print(\"New:\", model_results[-1].cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\")\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with All Features (Scaled, Hyperparameters Tuned)\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1505265b",
   "metadata": {},
   "source": [
    "Very Similar to our previous models scores. \n",
    "\n",
    "As we said previously, our model could be overfitting. One way to address is this is to remove features, specifically, ones that have small modeling coefficients. We did this using SelectFromModel.\n",
    "\n",
    "### SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(logreg_model)\n",
    "\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb09952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a default threshold \n",
    "thresh = selector.threshold_\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see how many features will be eliminated\n",
    "coefs = selector.estimator_.coef_\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[coefs > thresh].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c48545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(X_train.columns, selector.get_support()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9082d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(X, selector):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a selector, use the selector to choose\n",
    "    the most important columns\n",
    "    \"\"\"\n",
    "    imps = dict(zip(X.columns, selector.get_support()))\n",
    "    selected_array = selector.transform(X)\n",
    "    selected_df = pd.DataFrame(selected_array,\n",
    "                               columns=[col for col in X.columns if imps[col]],\n",
    "                               index=X.index)\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = select_important_features(X=X_train, selector=selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_sel = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\",max_iter=25)\n",
    "\n",
    "logreg_sel.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefe947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "# select_results = ModelWithCV(\n",
    "#                     logreg_sel, \n",
    "#                     'logreg_sel',\n",
    "#                     X_train_selected,\n",
    "#                     y_train\n",
    "# )\n",
    "\n",
    "# Plot both all_features vs new model\n",
    "#f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "# model_results[0].plot_cv(ax=axes[0])\n",
    "# select_results.plot_cv(ax=axes[1])\n",
    "\n",
    "#plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca681b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Old:\", all_features_cross_val_score)\n",
    "# print(\"New:\", select_results.cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffc152",
   "metadata": {},
   "source": [
    "Unfortunately, our final two models were taking too long to run. My kernal kept stopping. So we were not able to get our final models or run a final model evaluation at this time. \n",
    "\n",
    "With more time, there is a lot more I would have liked to do.  For starters, there were alot of \"unknown\"s in our data. I think that running an imputer to impute data into those features could've been very helpful. As seen, the \"Unknowns\" were ranked among the most important features. From this, we could then run though a decision tree again to find the most important features, allowing us to eliminate the unimportant or overinflating ones, and assigning proper weight to the important ones. I beleive doing all of this would've given us better results on our test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c45a5",
   "metadata": {},
   "source": [
    "# Final Model Evaluation\n",
    "\n",
    "Now that we have a final model, run X_test through all of the preprocessing steps so we can evaluate the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcecd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_no_transformations = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing indicators\n",
    "# X_test_mi = add_missing_indicator_columns(X_test_no_transformations, indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out values for imputation\n",
    "# X_test_numeric = X_test_mi[numeric_feature_names]\n",
    "# X_test_categorical = X_test_mi[categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdde662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out values for imputation\n",
    "# impute missing values\n",
    "# X_test_numeric = impute_missing_values(X_test_numeric, numeric_imputer)\n",
    "# X_test_categorical = impute_missing_values(X_test_categorical, categorical_imputer)\n",
    "# X_test_imputed = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n",
    "# X_test_new = X_test_mi.drop(numeric_feature_names + categorical_feature_names, axis=1)\n",
    "# X_test_final = pd.concat([X_test_imputed, X_test_new], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba022fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode categorical data\n",
    "# for categorical_feature in categorical_feature_names:\n",
    "#     X_test_final = encode_and_concat_feature(X_test_final,\n",
    "#                                        categorical_feature, encoders[categorical_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63463db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scale values\n",
    "# X_test_scaled = scale_values(X_test_final, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36eaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "# X_test_selected = select_important_features(X_test_scaled, selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = LogisticRegression(random_state=2021, solver=\"liblinear\", penalty=\"l1\")\n",
    "# final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# final_model.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ef9df",
   "metadata": {},
   "source": [
    "## Compare the past models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a way to categorize our different models\n",
    "# model_candidates = [\n",
    "#     {\n",
    "#         'name':'dummy_model'\n",
    "#         ,'model':dummy_model\n",
    "#         ,'X_test':X_test\n",
    "#         ,'y_test':y_test\n",
    "#     },\n",
    "#     {\n",
    "#         'name':'simple_logreg_model'\n",
    "#         ,'model':simple_logreg_model\n",
    "#         ,'X_test':X_test_no_transformations[[\"SibSp\", \"Parch\", \"Fare\"]]\n",
    "#         ,'y_test':y_test\n",
    "#     },\n",
    "#     {\n",
    "#         'name':'logreg_model_more_iterations'\n",
    "#         ,'model':logreg_model_more_iterations\n",
    "#         ,'X_test':X_test_final\n",
    "#         ,'y_test':y_test\n",
    "#     },\n",
    "#     {\n",
    "#         'name':'logreg_model_higher_tolerance'\n",
    "#         ,'model':logreg_model_higher_tolerance\n",
    "#         ,'X_test':X_test_final\n",
    "#         ,'y_test':y_test\n",
    "#     },\n",
    "#     {\n",
    "#         'name':'final_model'\n",
    "#         ,'model':final_model\n",
    "#         ,'X_test':X_test_selected\n",
    "#         ,'y_test':y_test\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_scores_dict = {\n",
    "#     \"Model Name\": [candidate.get('name') for candidate in model_candidates],\n",
    "#     \"Mean Accuracy\": [\n",
    "#         candidate.get('model').score(\n",
    "#                                 candidate.get('X_test'), \n",
    "#                                 candidate.get('y_test')\n",
    "#         ) \n",
    "#         for candidate in model_candidates\n",
    "#     ]\n",
    "    \n",
    "# }\n",
    "# final_scores_df = pd.DataFrame(final_scores_dict).set_index('Model Name')\n",
    "# final_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37927a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 2\n",
    "# ncols = math.ceil(len(model_candidates)/nrows)\n",
    "\n",
    "# fig, axes = plt.subplots(\n",
    "#                 nrows=nrows,\n",
    "#                 ncols=ncols,\n",
    "#                 figsize=(12, 6)\n",
    "# )\n",
    "# fig.suptitle(\"Confusion Matrix Comparison\")\n",
    "\n",
    "# # Turn off all the axes (in case nothing to plot); turn on while iterating over\n",
    "# [ax.axis('off') for ax in axes.ravel()]\n",
    "\n",
    "\n",
    "# for i,candidate in enumerate(model_candidates):\n",
    "#     # Logic for making rows and columns for matrices\n",
    "#     row = i // 3\n",
    "#     col = i % 3\n",
    "#     ax = axes[row][col]\n",
    "    \n",
    "#     ax.set_title(candidate.get('name'))\n",
    "#     ax.set_axis_on() \n",
    "#     cm_display = plot_confusion_matrix(\n",
    "#                     candidate.get('model'),\n",
    "#                     candidate.get('X_test'),\n",
    "#                     candidate.get('y_test'),\n",
    "#                     normalize='true',\n",
    "#                     cmap='plasma',\n",
    "#                     ax=ax,\n",
    "                    \n",
    "#     )\n",
    "#     cm_display.im_.set_clim(0, 1)\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a366b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot only the last models we created (so it's not too cluttered)\n",
    "# for model_candidate in model_candidates[3:]:\n",
    "#     plot_roc_curve(\n",
    "#         model_candidate.get('model'),\n",
    "#         model_candidate.get('X_test'),\n",
    "#         model_candidate.get('y_test'), \n",
    "#         name=model_candidate.get('name'),\n",
    "#         ax=ax\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d377497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the final model against the other earlier models\n",
    "# plot_roc_curve(\n",
    "#     final_model, \n",
    "#     X_test_selected, \n",
    "#     y_test,\n",
    "#     name='final_model', \n",
    "#     ax=ax\n",
    "# )\n",
    "\n",
    "# for model_candidate in model_candidates[:3]:\n",
    "#     plot_roc_curve(\n",
    "#         model_candidate.get('model'),\n",
    "#         model_candidate.get('X_test'),\n",
    "#         model_candidate.get('y_test'), \n",
    "#         name=model_candidate.get('name'),\n",
    "#         ax=ax\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
